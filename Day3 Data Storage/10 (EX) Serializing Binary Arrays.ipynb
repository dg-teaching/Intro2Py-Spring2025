{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Scientific Data to Binary Files\n",
    "\n",
    "Text files are nice for small files, but most scientific data is way too large and complex to be easy to work with as text.  So, we need more flexibility, and for that we use binary files, which allow us to store data however we want.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Arrays Review: Arrays of DTypes\n",
    "\n",
    "In scientific work, the most important kind of binary data are `arrays`.  Controlling how they are stored both on-disk and in-memory affects how precise, compact, and performant our analyses are.  Since all elements of an array are stored the same way (i.e. they have the same `dtype`), all values get processed the same way in calculations.\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`np.array([1, 2, 3])`** | Make an array from a list of values |\n",
    "| **`np.array(..., dtype=np.uint8)`** | Make an array of 8-bit, unsigned integers |\n",
    "| **`new_x = x.astype(np.uint8)`** | Make a copy of the array that has 8-bit, unsigned integers |\n",
    "| **`x.nbytes`** | How many bytes in memory an array takes up. |\n",
    "| **`x.size`** | How many elements an array contains. |\n",
    "\n",
    "What do these dtypes represent?  Here are some examples to help parse them:\n",
    "\n",
    "| DType | Value Stored | Supports Negative Numbers | N Bits | N Bytes |  Commonly Used With |\n",
    "| :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| **`float32`** | scientific | yes | 16 | 2 | GPU Math |\n",
    "| **`float64`** | scientific | yes | 64 | 8 | Any Decimal Numbers |\n",
    "| **`int64`** | whole numbers | yes | 16 | 2 | Any Whole Numbers |\n",
    "| **`uint8`** | whole numbers | no | 16 | 2 | Image Pixel Values for 8-bit images |\n",
    "| **`bool`** | true or false | no | 4 | 1 | Filtering Masks |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**: Let's practice making arrays with different dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Store the values `[1, 2, 3]` as 64-bit integers. How many bytes does the array take up in memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3], dtype=np.int64)\n",
    "x, x.dtype, x.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Store the values `[1, 2, 3]` as 16-bit integers. How many bytes does the array take up in memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: `float` are meant to store decimal values, and `int` is meant to store whole numbers.  What happens if a decimal value is stored as an `int`?  Store the values `[1.2, 3.4, 5.6, 7.8]` as 32-bit integers.  What happens to the values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: The fewer the bits, the smaller the range of values that can be stored in them.  Let's try exceeding a limit by store the values `[253, 254, 255, 256]` as unsigned, 8-bit integers.  What error do we get?  What's the highest number that can be stored as `uint8` values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: The fewer the bits, the worse the precision of floating point values.  Note the three calculations below: Which of the three is incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.float64(10) / np.float64(0.001), \\\n",
    "np.float32(10) / np.float32(0.001), \\\n",
    "np.float16(10) / np.float16(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing Numpy arrays directly to a binary File\n",
    "\n",
    "Can we create our own binary files?  Of course!  Below, we'll make numpy arrays and play around with serializing the numpy \n",
    "\n",
    "| **Code** | Description |\n",
    "| :-- | :-- |\n",
    "| **`x = np.arange(10).astype(np.uint8)`** | Create an array `x` of 10 values (0-9) where each is stored as unsigned, 8-bit integers |\n",
    "| <code>f = open('data.dat', 'wb')<br>f.write(x.tobytes())<br>f.close()</code> | Write the bytestring representation of the array to a binary file. |\n",
    "| <code>f = open('data.dat', 'rb')<br>x = np.frombuffer(f.read(), dtype=np.uint8)<br>f.close()</code> | Read the bytestring data from a binary file and interpret it as a numpy array. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Write 5 16-bit integers to the binary file `x1.dat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(5).astype(np.int16)\n",
    "f = open('x1.dat', 'wb')\n",
    "f.write(x.tobytes())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write 8 8-bit, unsigned integers to the binary file `x2.dat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the `x2.dat` file in your text editor (you'll have to insist that it's readable as text, even though it's really not).  What does the data look like, is the data recognizable?  More importantly, how many characters are there in the file, does this seem like the right number of bytes (Tip: You can count characters just by highlighting all the characters in the VSCode text editor and in the bottom-right corner it will say how many is selected.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write 8 64-bit, unsigned integers to the binary file `x3.dat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the `x3.dat` file in your text editor (you'll have to insist that it's readable as text, even though it's really not).  What does the data look like, is the data recognizable?  More importantly, does it seem like it's got **more** data than the `x2.dat` file?  It should take up 8x as much space, because 64 bits is 8 times more space than 8 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the `x2.dat` file back as a Numpy array, and make sure it was read back correctly  (note: you'll have to tell numpy what dtype the data should be read as). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the `x3.dat` file back as a Numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Numpy Arrays to NPY and NPZ Files using Numpy\n",
    "\n",
    "It's challenging to read in data files when you don't know ahead of time how the data is actually stored.  Numpy provides two formats, \".npy\" and \".npz\" that make reading data \n",
    "into Numpy easier by putting the data's format directly into the file itself.  This takes up just a little more space on disk, but it makes the data way easier to work with!\n",
    "\n",
    "| **Code** | **Description ** |\n",
    "| :-- | :-- |\n",
    "| **`np.random.random(size=100).astype(np.float32)`** | Create 100 random float32 values between 0 and 1 |\n",
    "| **`np.random.randint(1, 10, size=20).astype(np.uint8)`** | Create 20 random 8-bit, unsigned integers between 0 and 10 |\n",
    "| **`np.save('data.npy', data)`** | Save the `data` array to the `data.npy` file |\n",
    "| **`np.savez('data.npz', x=x1, y=y1)`** | Save the `x1` and `y1` arrays as `x` and `y` variables in the `data.npz` file |\n",
    "| **`data = np.load('data.npy')`** | Load the `data` array from the `data.npy` file |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Create 10 random 16-bit floating numbers and save them to `x1.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random(size=10).astype(np.float16)\n",
    "np.save('x1.npy', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Load the data in `x1.npy`.  Was it saved correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('x1.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 15 random 32-bit integers between 100 and 200 and save them to `x2.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in `x2.npy`.  was it saved correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a text editor, take a look at how the `x2.npy` file was saved. (Note: you may have to \"insist\" that the data can be read in the text editor). Can you find where the data's schema is stored in the file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save both an array of times (float64) and an array of voltages (uint16) to `ephys.npz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.array([.1, .2, .3, .4], dtype=np.float64)\n",
    "volts = np.array([300, 678, 426, 378], dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a text editor, take a look at how the `ephys.npz` file was saved. (Note: you may have to \"insist\" that the data can be read in the text editor). Can you find where the data's schema is stored in the file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the volts from `ephys.npz`.  Was it saved correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Compressing Data with `np.savez_compressed`\n",
    "\n",
    "Numpy comes with a function called `savez_compressed` that makes NPZ files and does Zip-compression on each variable.  This simple compression method can make quite a difference in how compact the data is stored on the computer, though how much will depend on the data itself.  \n",
    "\n",
    "Below are some examples of benchmarks on different datasets to get an idea of how the data affects things.  Note that both different data and different compression algorithms will have different performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO  # Used to simulate files\n",
    "from time import perf_counter  # Used to measure time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1**: All Values the Same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(1_000_000, dtype=np.int32)  # All values are 1\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Uncompressed.  File Size: {len(f.read())/1024:.1f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(1_000_000, dtype=np.int32)  # All values are 1\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez_compressed(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Compressed.  File Size: {len(f.read())/1024:.1f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Did compression make much of a difference in file size when all the values were the same?  What about in write time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2**: Ascending Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 1_000_000, dtype=np.int32)\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Uncompressed.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 1_000_000, dtype=np.int32)\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez_compressed(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Compressed.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Did compression make much of a difference in file size when the values were different, but changing in a simple pattern?  What about in write time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 3**: Random Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(1, 1_000_000, size=1_000_000, dtype=np.int32)  # random integers\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Uncompressed.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(1, 1_000_000, size=1_000_000, dtype=np.int32)  # random integers\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez_compressed(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Compressed.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Did compression make much of a difference in file size when all the values were random?  What about in write time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 4: Noisy Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1_000_000, dtype=np.int32) + np.random.randint(1, 20, size=1_000_000, dtype=np.int32)  \n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Uncompressed.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1_000_000, dtype=np.int32) + np.random.randint(1, 20, size=1_000_000, dtype=np.int32) \n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez_compressed(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Compressed.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Did compression make much of a difference in file size when the values had noise in them?  What about in write time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 5: Random-but-small-range.**  This data is completely noise, but only in the values 0-255, which is much fewer bits than a 32-int can represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 255, size=1_000_000, dtype=np.int32)  # random integers\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Uncompressed.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 255, size=1_000_000, dtype=np.int32)  # random integers\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez_compressed(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Compressed.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 255, size=1_000_000, dtype=np.uint8)  # random integers\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Uncompressed, but Optimal Dtype.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 255, size=1_000_000, dtype=np.uint8)  # random integers\n",
    "f = BytesIO()\n",
    "t0 = perf_counter()\n",
    "np.savez_compressed(f, x=x)\n",
    "t1 = perf_counter()\n",
    "f.seek(0)\n",
    "f\"Compressed, and Optimal Dtype.  File Size: {len(f.read())/1024:.2f} MB, Write Time: {(t1 - t0)*1000:.1f} msecs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Did compression make much of a difference in file size when the values were in a smaller range than needed?  What about in write time?  Did compressing remove the benefits of storing the data in an optimal dtype in the first place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Selecting Data \"Out-of-Core\" using Memory Mapping: `np.memmap`\n",
    "\n",
    "If you have a massive file on disk, but don't want it to take up a lot of space in RAM, one technique you can use is to read in only small part of your data at a time.  For example, on could read in one frame at a time of a video, or one channel at a time of a long multi-channel recording.  Numpy provides a technique called Memory Mapping, where it uses its knowledge of the expected bytes order of the file to read in only the needed data.\n",
    "\n",
    "Note that this saves a lot of memory, but slows down calculations compared to when the data is already loaded into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1**: A Simple Buffer of Data.  Here, all you need to know is the dtype and shape of the array to be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mydata2.dat', 'wb') as f:\n",
    "    f.write(np.arange(100, dtype=np.uint16).tobytes())\n",
    "\n",
    "x = np.memmap('mydata2.dat', dtype=np.uint16, shape=100)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2**: An NPY File.  Here, you need to know is the dtype and shape of the array to be read, plus how many bytes to skip before the data starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mydata3.npy\", np.arange(100, dtype=np.uint16))\n",
    "x = np.memmap('mydata3.npy', dtype=np.uint16, shape=100, offset=128)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 3:** A Compressed NPZ File.  As far as I know, this is **impossible** to memory map, as it cannot be known in advance which bytes in the file correspond to which values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"mydata3.npz\", x=np.arange(100, dtype=np.uint16))\n",
    "x = np.memmap('mydata3.npz', dtype=np.uint16, offset=1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
