{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install h5py numpy xarray netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Mixed-Type, Labelled, Data and Metadata to HDF5 Files using `h5py`\n",
    "\n",
    "HDF5 files have a lot more features:\n",
    "  - They are highly cross-platform and work with a wide variety of tools\n",
    "  - They can store many different datasets in a single file (or even in multiple linked files)\n",
    "  - They can store metadata alonside the data\n",
    "  - They let you store data hierarchically, making a nice dict-like nested organization for your data\n",
    "  - They can compress your data.\n",
    "  - They let you work with data that is larger than memory, making it easy to read in only the data that you need.\n",
    "  - They can be easily previewed and inspected using the https://myhdf5.hdfgroup.org/ web tool!\n",
    "  \n",
    "So many features!  Here, we'll get a basic senses of how they work by using the `h5py` library, which gives us a dict-like, Numpy-native interface to HDF5 files and is used internally by many popular Python frameworks.\n",
    "\n",
    "\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f = h5py.File('filename.h5', 'w')`** | Open an h5py file object for writing |\n",
    "| **`f.close()`** | Closes the h5py file and releases the linked file back to the operating system. |\n",
    "| **`f.create_dataset('temp', data=x)`** | Write an array called 'temp' with the data in the numy array `x` into the HDF5 file |\n",
    "| **`f.create_dataset('data/temp', data=x)`** | Write an array called 'temp' in the folder called \"data\" with the data in the numy array `x` into the HDF5 file |\n",
    "| **`f.attrs['name'] = 'Session 1'`** | Set an attribute as metadata onto the root group of the HDF5 file -- this works like a normal Python dictionary |\n",
    "| **`f['x'].attrs['id'] = 'ABC'`** | Set an attribute as metadata onto the 'x' node of the HDF file |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Storing Arrays in HDF5 as \"Datasets\" and Organizing them in \"Groups\"\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f = h5py.File('filename.h5', 'w')`** | Open an h5py file object for writing |\n",
    "| **`f.close()`** | Closes the h5py file and releases the linked file back to the operating system. |\n",
    "| **`f['x'] = np.array([1, 2, 3])`** | Put a Numpy array into a **dataset** called `x` |\n",
    "| **`f['folder/x'] = np.array([1, 2, 3])`** | Put a Numpy array into  **dataset** called `x`, in a **group** called `folder` |\n",
    "| **`f.create_dataset('folder/x', shape=(100,2), dtype=np.uint8)`** | Make an empty dataset in the file |\n",
    "| **`f.create_dataset('folder/x', data=my_array, compression='gzip')`** | Store a numpy array as a dataset using a given compression algorithm. |\n",
    "| **`f['x'][:]`** | Read in the dataset into a Numpy array. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Open and Close a File**.  Open an HDF5 file named `exercise1.h5` for storing EEG data, then close it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('exercise1.h5', 'w') as f:\n",
    "    f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise: Store a Small Array of Spike Counts**. Store a small 1D NumPy array of spike counts in a file called `exercise2.h5`, in a dataset called `spike_counts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Read Back the Spike Counts**.  Read the `spike_counts` dataset from `exercise2.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Create a Dataset Inside a Group for LFP Data**. Create a dataset named `lfp_data` inside a group called `session1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Read the LFP Dataset from the Folder**.  Read the `lfp_data` from the `session1` group in the previous exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Create an Empty Dataset for Neuron Responses**. Create an empty dataset called `neuron_responses` with shape (100, 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Compressed Dataset for EEG**.  Create a compressed dataset called `eeg_data` using gzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Read the Compressed EEG Data**. Read the `eeg_data` dataset from the previous exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Put Multiple Datasets into a Schema**:  Create a file with the following organization:\n",
    "\n",
    "  - behavior/\n",
    "    - trial: 10 values, uint8\n",
    "    - time: 100 values,  float32 \n",
    "    - lick_rate: 10 x 100 values, float32\n",
    "  - ephys/\n",
    "    - spike_times: 50 values, float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Metadata in HDF5 as \"Attributes\"\n",
    "\n",
    "Labeling our data and keeping those labels together with the data itself is a key practice in having well-organized data, and hdf5 makes it easy to put labels directly inside the files.  \n",
    "\n",
    "One practice to be aware of:  \n",
    "  - General metadata for the file is often attached to the \"root\" group, so it's easy to find.  Things like experiment-level or session-level metadata are often stored there.  \n",
    "  - Metadata that describes a specific array are often attached to the hdf5 dataset itself.  Things like dimension labels, units, text descriptions of what the data represents can usually be found here.\n",
    "\n",
    ".\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f.attrs['subject'] = 'Doug'`** | Add a \"subject\" attribute to the root-level group. |\n",
    "| **`f['x'].attrs['subject'] = 'Doug'`** | Add a \"subject\" attribute to the 'x' group or dataset. |\n",
    "| **`dict(f.attrs)`** | Get all the attributes attached to the root-level group as a dict.\n",
    "| **`dict(f['x'].attrs)`** | Get all the attributes attached to the 'x' group or dataset as a dict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a file with the following schema:\n",
    "\n",
    "```\n",
    "meta_ex1.h5\n",
    "└─ / (root)\n",
    "   └─ subject_id = \"RatA\"  (attribute)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Reopen `meta_ex1.h5` and get the \"subject_id\" attribute value from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a file with the following schema:\n",
    "```\n",
    "meta_ex2.h5\n",
    "└─ / (root)\n",
    "   └─ lfp_data (dataset, shape=(100,))\n",
    "      └─ recording_date = \"2025-03-31\"  (attribute)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Reopen `meta_ex2.h5` and the `recording_date` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a file with the following schema:\n",
    "\n",
    "```\n",
    "meta_ex3.h5\n",
    "└─ / (root)\n",
    "   ├─ experimenter = \"Dr. Gray\"       (attribute)\n",
    "   ├─ experiment_type = \"Optogenetics\" (attribute)\n",
    "   └─ lab = \"NeuroLab\"                (attribute)\n",
    "   └─ spike_trains (dataset, shape=(4,50))\n",
    "      ├─ brain_region = \"Hippocampus\"   (attribute)\n",
    "      └─ num_channels = 4              (attribute)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Reopen `meta_ex3.h5` and get all the root-level metadata into a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Labeling Dimensions of Datasets by Linking Arrays\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f['temperature'].dims[0].label = 'time'`** | Mark the first dimension of the `temperature` dataset as going over time. |\n",
    "| **`f['time'].make_scale('Time')`** | Label the `time` dataset as a dimension scale with name \"Time\". |\n",
    "| **`f['temperature'].dims[0].attach_scale(f['time'])`** | Link each value of `time` to each value of `temperature`. |\n",
    "| **`xr.load_dataset('myfile.h5')`** | XArray will recognize and organize linked datasets automatically. |\n",
    "\n",
    "Reference: https://docs.h5py.org/en/stable/high/dims.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example: Single-Dimension LFP Data Over Time\n",
    "- **Goal**: Create a file named `ex1.h5` with:\n",
    "  - A 1D dataset called `lfp_data` (e.g., shape `(1000,)`).\n",
    "  - A 1D dataset called `time` (same length, storing time points).\n",
    "  - Make `time` a dimension scale named `\"Time\"`.\n",
    "  - Attach this scale to the first (and only) dimension of `lfp_data`.\n",
    "  - Label that dimension `\"time\"`.\n",
    "- **Check**: Call `xr.load_dataset('ex1.h5')` and confirm it recognizes `lfp_data` with a coordinate named `\"time\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "with h5py.File('ex1.h5', 'w') as f:\n",
    "    f['time'] = np.linspace(0, 10, 1000)  # 1000 data points spanning 0 to 10 seconds\n",
    "    f['time'].make_scale('Time')  # Mark the 'time' dataset as a dimension scale named \"Time\"\n",
    "\n",
    "    f['lfp_data'] = np.random.randn(1000) # 1000 normally-distributed random values.\n",
    "    f['lfp_data'].dims[0].label = 'time'  # # Label the first dimension as \"time\"\n",
    "    f['lfp_data'].dims[0].attach_scale(f['time'])\n",
    "\n",
    "xr.load_dataset('ex1.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Two-Dimensional LFP Data (Time × Channels)\n",
    "- **Goal**: Create a file named `ex2.h5` with:\n",
    "  - A 1D dataset `time` (shape `(1000,)`) representing time points.\n",
    "  - A 1D dataset `channels` (shape `(16,)`) representing electrode channels.\n",
    "  - A 2D dataset `lfp_data` (shape `(1000, 16)`).\n",
    "  - Make `time` and `channels` dimension scales (`\"Time\"` and `\"Channels\"`).\n",
    "  - Attach them as dimension scales to `lfp_data` on dim 0 and dim 1, respectively.\n",
    "  - Label these dimensions `\"time\"` and `\"channel\"`.\n",
    "- **Check**: `xr.load_dataset('ex2.h5')` should show `lfp_data` with coordinates `(\"time\", \"channel\")`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 3: ROI Signals (Time × ROI)\n",
    "- **Goal**: Create `ex3.h5` with:\n",
    "  - A 1D dataset `time` (e.g., `(1200,)`) for frames in a calcium imaging experiment.\n",
    "  - A 1D dataset `roi` (e.g., `(5,)`) for different regions of interest.\n",
    "  - A 2D dataset `roi_signals` (shape `(1200, 5)`).\n",
    "  - Make and attach scales with names `\"Time\"` for `time` and `\"ROI\"` for `roi`.\n",
    "  - Label dimensions as `\"time\"` and `\"roi\"`.\n",
    "- **Check**: `xr.load_dataset('ex3.h5')` should yield `roi_signals` with xarray dims `[\"time\", \"roi\"]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Imaging Data (Time × X × Y)\n",
    "- **Goal**: Create `ex6.h5` with:\n",
    "  - A 1D dataset `time` (e.g., `(50,)`) for imaging frames.\n",
    "  - A 1D dataset `x` (e.g., `(64,)`) for X-coordinates in the image.\n",
    "  - A 1D dataset `y` (e.g., `(64,)`) for Y-coordinates in the image.\n",
    "  - A 3D dataset `image_stack` (shape `(50, 64, 64)`) for pixel intensities.\n",
    "  - Make `time`, `x`, and `y` dimension scales named `\"Time\"`, `\"X\"`, `\"Y\"`.\n",
    "  - Attach them to the corresponding dimensions of `image_stack`.\n",
    "  - Label these dimensions `\"time\"`, `\"x\"`, and `\"y\"`.\n",
    "- **Check**: `xr.load_dataset('ex6.h5')` should load `image_stack` with three named dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
